# Enterprise AI Accelerator
## Day 1 - Models and Retrieval Augmented Generation (RAG)
## Session labs 
## Revision 1.1 - 11/22/25

**Follow the startup instructions in the README.md file IF NOT ALREADY DONE!**

**NOTE: To copy and paste in the codespace, you may need to use keyboard commands - CTRL-C and CTRL-V. Chrome may work best for this.**

**Lab 1 - Working with Neural Networks**

**Purpose: In this lab, we’ll learn more about neural networks by seeing how one is coded and trained.**

1. In our repository, we have a set of Python programs to help us illustrate and work with concepts in the labs. The first set are in the *llm* subdirectory. Go to the *TERMINAL* tab in the bottom part of your codespace and change into that directory.

```
cd llm
```
<br><br>

2. For this lab, we have a simple neural net coded in Python. The file name is nn.py. Open the file either by clicking on [**llm/nn.py**](./llm/nn.py) or by entering the command below in the codespace's terminal.

```
code nn.py
```
<br><br>


3. Scroll down to around line 55. Notice the *training_inputs* data and the *training_outputs* data. Each row of the *training_outputs* is what we want the model to predict for the corresponding input row. As coded, the output for the sample inputs ends up being the same as the first element of the array.  For inputs [0,0,1] we are trying to train the model to predict [0]. For the inputs [1,0,1], we are trying to train the model to predict [1], etc. The table below may help to explain.

| **Dataset** | **Values** | **Desired Prediction** |
| :---------: | :--------: | :--------------------: |
| **1** |  0  0  1  |            0           |
| **2** |  1  1  1  |            1           |
| **3** |  1  0  1  |            1           |
| **4** |  0  1  1  |            0           |

![Code in simple nn](./images/aia-1-3.png?raw=true "Code in simple nn")

<br><br>

4. When we run the program, it will train the neural net to try and predict the outputs corresponding to the inputs. You will see the random training weights to start and then the adjusted weights to make the model predict the output. You will then be prompted to put in your own training data. We'll look at that in the next step. For now, go ahead and run the program (command below) but don't put in any inputs yet. Just notice how the weights have been adjusted after the training process.

```
python nn.py
```
![Starting run of simple nn](./images/aia-1-4.png?raw=true "Starting run of simple nn") 

<br><br>

5. What you should see is that the weights after training are now set in a way that makes it more likely that the result will match the expected output value. (The higher positive value for the first weight means that the model has looked at the training data and realized it should "weigh" the first input higher in its prediction.) To prove this out, you can enter your own input set - just use 1's and 0's for each input. 

![Inputs to simple nn](./images/aia-1-5.png?raw=true "Inputs to simple nn") 

<br><br>

6. After you put in your inputs, the neural net will process your input and because of the training, it should predict a result that is close to the first input value you entered (the one for *Input one*).

![Prediction close to first input](./images/aia-1-6.png?raw=true "Prediction close to first input") 

<br><br>

7. Now, let's see what happens if we change the expected outputs to be different. In the editor for the genai_nn.py file, find the line for the *training_outputs*. Modify the values in the array to be ([[0],[1],[0],[1]]). These are the values of the second element in each of the training data entries. After you're done, save your changes. (You can use the CMD/CTRL+S keyboard shortcut.)

![Modifying expected outputs](./images/aia-1-7.png?raw=true "Modifying expected outputs")

<br><br>

8. Now, run the neural net again. This time when the weights after training are shown, you should see a bias for a higher weight for the second item.
```
python nn.py
```
![Second run of simple nn](./images/aia-1-8.png?raw=true "Second run of simple nn") 

<br><br>

9. At the input prompts, just input any sequence of 0's and 1's as before.

<br><br>

10. When the trained model then processes your inputs, you should see that it predicts a value that is close to 0 or 1 depending on what your second input was.

![Second output of simple nn](./images/aia-1-9.png?raw=true "Second output of simple nn")

<br><br>

11. (Optional) If you get done early and want more to do, feel free to try other combinations of training inputs and training outputs.
    
<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 2 - Experimenting with Tokenizations**

**Purpose: In this lab, we'll see how different models do tokenization.**

1. In the same *llm* directory, we have a simple program that can load a model and print out tokens generated by it. The file name is *tokenizer.py*. You can view the file either by clicking on [**llm/tokenizer.py**](./llm/tokenizer.py) or by entering the command below in the codespace's terminal (assuming you're still in the *genai* directory).

```
code tokenizer.py
```

<br><br>

2. This program can be run and passed a model to use for tokenization. To start, we'll be using a model named *bert-base-uncased*. Let's look at this model on huggingface.co.  Go to https://huggingface.co/models and in the *Models* search area, type in *bert-base-uncased*. Select the entry for *google-bert/bert-base-uncased*.

![Finding bert model on huggingface](./images/aia-1-10.png?raw=true "Finding bert model on huggingface")

<br><br>

3. Once you click on the selection, you'll be on the *model card* tab for the model. Take a look at the model card for the model and then click on the *Files and Versions* and *Community* tabs to look at those pages.

![huggingface tabs](./images/aia-1-11.png?raw=true "huggingface tabs")

<br><br>

4. Now let's switch back to the codespace and, in the terminal, run the *tokenizer* program with the *bert-base-uncased* model. Enter the command below. This will download some of the files you saw on the *Files* tab for the model in HuggingFace.

```
python tokenizer.py bert-base-uncased
```

<br><br>

5. After the program starts, you will be at a prompt to *Enter text*. Enter in some text like the following to see how it will be tokenized.

```
This is sample text for tokenization and text for embeddings.
```

![input for tokenization](./images/aia-1-12.png?raw=true "input for tokenization")

<br><br>

6. After you enter this, you'll see the various subword tokens that were extracted from the text you entered. And you'll also see the ids for the tokens stored in the model that matched the subwords.

![tokenization output](./images/aia-1-13.png?raw=true "tokenization output")

<br><br>

7. Next, you can try out some other models by repeating steps 4 - 6 for other tokenizers like the following. (You can use the same text string or different ones. Notice how the text is broken down depending on the model and also the meta-characters.)
```
python tokenizer.py roberta-base
python tokenizer.py gpt2
python tokenizer.py xlnet-large-cased
```

<br><br>

8. (Optional) If you finish early and want more to do, you can look up the models from step 7 on huggingface.co/models.

<br>  
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 3 - Understanding embeddings, vectors and similarity measures**

**Purpose: In this lab, we'll see how tokens get mapped to vectors and how vectors can be compared.**

1. In the repository, we have a Python program that uses a Tokenizer and Model to create embeddings for three terms that you input. It then computes and displays the cosine similarity between each combination. Open the file to look at it by clicking on [**llm/vectors.py**](./llm/vectors.py) or by using the command below in the terminal.


```
code vectors.py
```
<br><br>

2. Let's run the program. As we did for the tokenizer example, we'll pass in a model to use. We'll also pass in a second argument which is the number of dimensions from the vector for each term to show. Run the program with the command below. You can wait to enter terms until the next step.


```
python vectors.py bert-base-cased 5
```

![vectors program run](./images/aia-1-14.png?raw=true "vectors program run")

<br><br>

3. The command we just ran loads up the bert-base-cased model and tells it to show the first 5 dimensions of each vector for the terms we enter. The program will be prompting you for three terms. Enter each one in turn. You can try two closely related words and one that is not closely related. For example
   - king
   - queen
   - duck

![vectors program inputs](./images/aia-1-15.png?raw=true "vectors program inputs")

<br><br>

4. Once you enter the terms, you'll see the first 5 dimensions for each term. And then you'll see the cosine similarity displayed between each possible pair. This is how similar each pair of words is. The two that are most similar should have a higher cosine similarity "score".

![vectors program outputs](./images/aia-1-16.png?raw=true "vectors program outputs")

<br><br>

5. Each vector in the bert-based models have 768 dimensions. Let's run the program again and tell it to display 768 dimensions for each of the three terms.  Also, you can try another set of terms that are more closely related, like *multiplication*, *division*, *addition*.

```
python vectors.py bert-base-cased 768
```

<br><br>

6. You should see that the cosine similarities for all pair combinations are not as far apart this time.

![vectors program second outputs](./images/aia-1-17.png?raw=true "vectors program second outputs")

<br><br>

7. As part of the output from the program, you'll also see the *token id* for each term. (It is above the print of the dimensions. If you don't want to scroll through all the dimensions, you can just run it again with a small number of dimensions like we did in step 2.) If you're using the same model as you did in lab 2 for tokenization, the ids will be the same. 

![token id](./images/aia-1-18.png?raw=true "token id")

<br><br>


8. You can actually see where these mappings are stored if you look at the model on Hugging Face. For instance, for the *bert-base-cased* model, you can go to https://huggingface.co and search for bert-base-cased. Select the entry for google-bert/bert-base-cased. (Make sure you pick the one with that name.)

![finding model](./images/aia-1-19.png?raw=true "finding model")

<br><br>

9. On the page for the model, click on the *Files and versions* tab. Then find the file *tokenizer.json* and click on it. The file will be too large to display, so click on the *check the raw version* link to see the actual content.

![selecting tokenizer.json](./images/aia-1-20.png?raw=true "selecting tokenizer.json")
![opening file](./images/aia-1-21.png?raw=true "opening file")

<br><br>

9. You can search for the terms you entered previously with a Ctrl-F or Cmd-F and find the mapping between the term and the id. If you look for "##" you'll see mappings for parts of tokens like you may have seen in lab 2.

![finding terms in file](./images/aia-1-22.png?raw=true "finding terms in files")

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 4 - Working with transformer models**

**Purpose: In this lab, we’ll see how to interact with various models for different standard tasks**

1. In our repository, we have several different Python programs that utilize transformer models for standard types of LLM tasks. These programs have some random facts from different categories stored in them to use for transformer models to act on.

<br><br>

2. One of the programs is a simple translation example. The file name is *translation.py*. Open the file either by clicking on [**llm/translation.py**](./llm/translation.py) or by entering the command below in the codespace's terminal. 

```
code translation.py
```
<br><br>

3. Take a look at the file contents.  Notice that we are pulling in a specific model ending with 'en-fr'. This is a clue that this model is trained for English to French translation. Let's find out more about it. In a browser, go to *https://huggingface.co/models* and search for the model name 'Helsinki-NLP/opus-mt-en-fr' (or you can just go to huggingface.co/Helsinki-NLP/opus-mt-en-fr).

![model search](./images/aia-1-23.png?raw=true "model search")

<br><br>

3. You can look around on the model card for more info about the model. Notice that it has links to an *OPUS readme* and also links to download its original weights, translation test sets, etc.

<br><br>

4. When done looking around, go back to the repository and look at the rest of the *translation.py* file. What we are doing is loading the model, the tokenizer, and then taking a set of random texts and running them through the tokenizer and model to do the translation. Go ahead and execute the code in the terminal via the command below.

```
python translation.py
```

![translation by model](./images/aia-1-24.png?raw=true "translation by model")
 
<br><br>

5. There's also an example program for doing classification. The file name is classification.py. Open the file either by clicking on [**llm/classification.py**](./llm/classification.py) or by entering the command below in the codespace's terminal.


```
code classification.py
```

<br><br>

6. Take a look at the model for this one *joeddav/xlm-roberta-large-xnli* on huggingface.co and read about it. When done, come back to the repo.

<br><br>

7. *classification.py* uses a HuggingFace pipeline to do the main work. Notice it also includes a list of categories as *candidate_labels* that it will use to try and classify the data. Go ahead and run it to see it in action. (This will take awhile to download the model.) After it runs, you will see each topic, followed by the ratings for each category. The scores reflect how well the model thinks the topic fits a category. The highest score reflects which category the model thinks fit best.

```
python classification.py
```

![classification by model](./images/aia-1-25.png?raw=true "classification by model")

<br><br>

8. Finally, we have a program to do sentiment analysis. The file name is sentiment.py. Open the file either by clicking on [**llm/sentiment.py**](./llm/sentiment.py) or by entering the command below in the codespace's terminal.

```
code sentiment.py
```

<br><br>

9. Again, you can look at the model used by this one *distilbert-base-uncased-finetuned-sst-2-english* in Hugging Face.

<br><br>

10. When ready, go ahead and run this one in the similar way and observe which ones it classified as positive and which as negative and the relative scores.

```
python sentiment.py
```

![sentiment by model](./images/aia-1-26.png?raw=true "sentiment by model")

<br><br>

11. If you're done early, feel free to change the texts, the candidate_labels in the previous model, etc. and rerun the models to see the results.

<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 5 - Fine-tuning Models**

**Purpose: In this lab, we'll see how we can fine-tune a model with a set of data to get better responses in a particular domain.**

1. For this lab, we will build out a starter file into a full fine-tuning example. The starter file is in the ft (for "fine tuning") directory. Change into that directory.

```
cd /workspaces/aia-day1/ft
```

2. We'll be tuning a basic model from Hugging Face called Distilbert. You can see information about that model here: <insert url>.

3. We'll be using that model to do sentiment analysis on Amazon product reviews. Sentiment analysis means determining if a review is positive, negative, or other - as we did with one of the examples in Lab 4. The dataset we'll be using for testing and fine-tuning is <insert data set>. You can see information about that dataset here: <insert URL>


4. To build out this file, we'll use a side-by-side compare and merge approach. That means we'll start up an editor with a completed version of the file on the left and the starter version on the right. To do this, run the command below: (the complete code is in extra/reviews-ft.txt)

```
code -d ../extra/reviews-ft.txt reviews-ft.py
```

5. After running this command, you'll see the side-by-side editors. Now we want to merge in the sections that are in the left file into the right file to build out our demo. Before you merge in a section, make sure to glance at the block of code to try and understand what it's doing. Then, when ready, hover over the bar between the two versions and an arrow should display. Click on the arrow to merge that section in.


6. Proceed down through the remaining differences, quickly reviewing the code to be merged, and then merging it with the arrow. Once you are done, the files should show as the same without any remaining "blocks" of differences. When done, close the diff view by clicking on the "X" in the tab at the top.


7.  Now we can run the demo with the following command:

```
python reviews-ft.py
```

8. This will first download the Distilbert model and then run through a subset of reviews to see how well the model does (without any fine-tuning) on determining the sentiment of each review. If this goes by too fast, you can scroll back up to see the reviews and results. You will probably see something in the 30-50% success range.

9. Now the model will use some Hugging Face transformer library tools to train the model from the dataset. This part will take probably as much as 5-8 minutes. What you are looking for here is the *loss value* to go down as the fine tuning proceeds. The loss value going down means that the model is getting better at predicting the sentiment as it learns from the dataset examples. 


10. At the end of the fine-tuning, the program will once again run through a test of the model's prediction for sentiments on the reviews. This time, since it has been fine-tuned, it should report success more in the 80-90% range.

<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 6 - Working with Vector Databases**

**Purpose: In this lab, we’ll learn about how to use vector databases for storing supporting data and doing similarity searches.**

1. For this lab and the next one, we have a data file that we'll be using that contains a list of office information and details for a ficticious company. The file is in [**data/offices.pdf**](./data/offices.pdf). You can use the link to open it and take a look at it.

![PDF data file](./images/31ai23.png?raw=true "PDF data file") 

2. In our repository, we have some simple tools built around a popular vector database called Chroma. There are two files which will create a vector db (index) for the *.py files in our repo and another to do the same for the office pdf. You can look at the files either via the usual "code <filename>" method or clicking on [**tools/index_code.py**](./tools/index_code.py) or [**tools/index_pdf.py**](./tools/index_pdf.py).

```
code ../tools/index_code.py
code ../tools/index_pdf.py
```

3. Let's create a vector database of our local python files. Run the program to index those. **This may run for a while before you see things happening.** You'll see the program loading the embedding model that will turn the code chunks into numeric represenations in the vector database and then it will read and index our *.py files. It will create a new local vector database in *./chroma_db*.

```
python ../tools/index_code.py
```

![Running code indexer](./images/gaidd88.png?raw=true "Running code indexer")

4. To help us do easy/simple searches against our vector databases, we have another tool at [**tools/search.py**](./tools/search.py). This tool connects to the ChromaDB vector database we create, and, using cosine similarity metrics, finds the top "hits" (matching chunks) and prints them out. You can open it and look at the code in the usual way if you want. No changes are needed to the code.

```
code ../tools/search.py
```

5. Now, let's run the search tool against the vector database we built in step 3. You can prompt it with phrases related to our coding like any of the ones shown below. When done, just type "exit".  Notice the top hits and their respective cosine similarity values. Are they close? Farther apart?

```
python ../tools/search.py

convert celsius to farenheit
embed model sentence-transformers
```

![Running search](./images/gaidd89.png?raw=true "Running search")

6.  Now, let's recreate our vector database based off of the PDF file. Just run the indexer for the pdf file.

```
python ../tools/index_pdf.py
```

![Indexing PDF](./images/gaidd90.png?raw=true "Indexing PDF")

7. Now, we can run the same search tool to find the top hits for information about offices. Below are some prompts you can try here. Note that in some of them, we're using keywords only found in the PDF document. Notice the cosine similarity values on each - are they close? Farther apart?  When done, just type "exit".

```
python ../tools/search.py

Queries:
Corporate Operations office
Seaside cities
Tech Development sites
High revenue branch
```

![PDF search](./images/gaidd95.png?raw=true "PDF search")

8. Keep in mind that this is not trying to intelligently answer your prompts at this point. This is a simple semantic search to find related chunks. In lab 8, we'll add in the LLM to give us better responses. In preparation for that lab, make sure that indexing for the PDF is the last one you ran and not the indexing for the Python files.


<p align="center">
**[END OF LAB]**
</p>
</br></br>


**Lab 7: Building a Basic RAG System**

**Purpose: In this lab, we'll create a basic RAG (Retrieval-Augmented Generation) system that can read PDF documents, store them in a vector database, and retrieve relevant information based on queries.**

1. As we did before, we'll be using the side-by-side editor merge approach in this lab to build out the code with our rag functionality.

2. First, let's navigate to our *rag* directory where we have the starter file and some utility files.

```
cd /workspaces/aia-day1/rag
```
<br><br>


3. Now, let's examine our basic RAG implementation. We have a completed version and a skeleton version. Use the diff command to see the differences:

```
code -d ../extra/rag_complete.txt rag_code.py
```
<br><br>

4. Once you have the diff view open, merge the code segments from the complete file (left side) into the skeleton file (right side) by clicking the arrow pointing right in the middle bar for each difference. Start with the imports section, then the document loading function, and finally the search functionality.

![Side-by-side merge](../images/merge-example.png)
<br><br>

5. After merging all the changes, close the diff view by clicking the "X" in the tab. Now let's test our basic RAG system:

```
python rag_code.py
```
<br><br>

6. The system should load the PDF documents and create the vector database. Let's now create a simple test script to query the knowledge base. Create a new file:

```
code rag_test.py
```
<br><br>

7. Paste the following code into the rag_test.py file:

```python
from rag_skeleton import KnowledgeBase

# Initialize the knowledge base
kb = KnowledgeBase()

# Test queries
queries = [
    "How do I return a product?",
    "What are the shipping options?",
    "How can I reset my password?"
]

print("Testing RAG System\n" + "="*50)
for query in queries:
    print(f"\nQuery: {query}")
    results = kb.search(query, max_results=2)

    for i, result in enumerate(results, 1):
        print(f"Result {i}: {result['content'][:200]}...")
        print(f"Category: {result['category']}, Score: {result['score']:.2f}")
```
<br><br>

8. Save the file (CTRL/CMD + S) and run the test:

```
python rag_test.py
```
<br><br>

9. You should see search results for each query. Notice how the system finds relevant documents based on the query content. Let's add some performance monitoring to our RAG system. Use the diff tool again:

```
code -d ../extra/rag_enhanced.txt rag_code.py
```
<br><br>

10. Merge in the enhancements that add timing information and better logging. After merging, close the diff view.
<br><br>

11. Now let's test the enhanced version with a simple benchmark:

```
python benchmark_rag.py
```
<br><br>

12. You should see timing information for document loading and search operations. This gives us a baseline for our RAG system performance.

![RAG Performance](../images/rag-performance.png)
<br><br>

13. Finally, let's verify that our knowledge base persists correctly. Run the persistence test:

```
python test_persistence.py
```
<br><br>

14. The test should show that documents are correctly stored and can be retrieved even after restarting the system.


**Key Takeaways:**
- You've built a basic RAG system that can load PDF documents
- The system uses ChromaDB as a vector database for similarity search
- Documents are chunked and embedded for efficient retrieval
- The system can find relevant information based on semantic similarity

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>


<p align="center">
<b>For educational use only by the attendees of our workshops.</b>
</p>

<p align="center">
<b>(c) 2025 Tech Skills Transformations and Brent C. Laster. All rights reserved.</b>
</p>
